{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirements ['requests>=2.32.2', 'tqdm>=4.66.3'] not found, attempting AutoUpdate...\n",
      "\u001b[31m\u001b[1mrequirements:\u001b[0m  AutoUpdate skipped (offline)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\frend/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2025-1-8 Python-3.12.1 torch-2.2.2+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!!!!! Alarme !!!!!\n",
      "!!!!! Alarme !!!!!\n",
      "!!!!! Alarme !!!!!\n",
      "!!!!! Alarme !!!!!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Charger le modèle YOLOv5 pré-entraîné\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s')  # Version \"small\" pour rapidité\n",
    "\n",
    "# Fonction de Filtrage et Comptage\n",
    "def filter_objects(detections, category, size_threshold, region):\n",
    "    \"\"\"\n",
    "    Filtrer les objets en fonction de la catégorie et de la taille minimale.\n",
    "    \n",
    "    :param detections: Résultats de détection (liste de dicts)\n",
    "    :param category: Catégorie à détecter (str)\n",
    "    :param size_threshold: Aire minimale de l'objet (int)\n",
    "    :return: Liste des objets filtrés\n",
    "    \"\"\"\n",
    "    filtered_objects = []\n",
    "    h, w, _ = frame.shape #detecter \n",
    "    for det in detections:\n",
    "        if det['name'] == category:\n",
    "            bbox_width = det['xmax'] - det['xmin']\n",
    "            bbox_height = det['ymax'] - det['ymin']\n",
    "            area = bbox_width * bbox_height\n",
    "            if area > size_threshold:\n",
    "                #detecter les objet dans la region top\n",
    "                if region == \"top\" and det['ymin'] < h / 2:\n",
    "                    filtered_objects.append(det)\n",
    "    return filtered_objects\n",
    "\n",
    "# Implémentation de l'Alarme\n",
    "def trigger_alarm(count, threshold):\n",
    "    \"\"\"\n",
    "    Déclenche une alarme si le seuil est dépassé.\n",
    "    \n",
    "    :param count: Nombre d'objets détectés (int)\n",
    "    :param threshold: Seuil à dépasser (int)\n",
    "    \"\"\"\n",
    "    if count >= threshold:\n",
    "        print(\"!!!!! Alarme !!!!!\")\n",
    "        # On ajoute même un petit bip sonore\n",
    "        duration = 1000  # ms\n",
    "        freq = 440  # Hz\n",
    "        try:\n",
    "            import winsound\n",
    "            winsound.Beep(freq, duration)\n",
    "        except ImportError:\n",
    "            print(\"Bip sonore non disponible sur cette plateforme.\")\n",
    "\n",
    "# Boucle de Détection Temps Réel\n",
    "# Paramètres, à régler\n",
    "CATEGORY = 'cell phone'  # Exemple : un crayon\n",
    "SIZE_THRESHOLD = 1000  # Aire minimale\n",
    "THRESHOLD = 2  # Seuil d'alarme déclancher quand il détécte un téléphone\n",
    "\n",
    "# Dictionnaire pour le comptage des objets\n",
    "object_count = defaultdict(int)\n",
    "\n",
    "# Capture vidéo depuis la webcam (à modifier pour lire une vidéo si webcam non disponible)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir l'image pour YOLO\n",
    "    results = model(frame)\n",
    "    detections = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "\n",
    "    # Filtrer les objets (par exemple, en prenant les objets à gauche)\n",
    "    filtered_objects = filter_objects(detections, CATEGORY, SIZE_THRESHOLD, region=\"top\")\n",
    "\n",
    "    # Mise à jour du comptage\n",
    "    object_count[CATEGORY] = len(filtered_objects)\n",
    "\n",
    "    # Déclencher l'alarme si nécessaire\n",
    "    trigger_alarm(object_count[CATEGORY], THRESHOLD)\n",
    "\n",
    "    # Annoter l'image\n",
    "    for obj in filtered_objects:\n",
    "        cv2.rectangle(frame, (int(obj['xmin']), int(obj['ymin'])), \n",
    "                             (int(obj['xmax']), int(obj['ymax'])), \n",
    "                      (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{obj['name']} {obj['confidence']:.2f}\", \n",
    "                    (int(obj['xmin']), int(obj['ymin']) - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Afficher le flux vidéo\n",
    "    cv2.imshow('Detection Objets', frame)\n",
    "\n",
    "    # Quitter avec la touche 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    #\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    out = cv2.VideoWriter('test2.avi', fourcc, 20.0, (640, 480))\n",
    "    out.write(frame)\n",
    "\n",
    "# Libérer les ressources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n"
     ]
    }
   ],
   "source": [
    "print(model.names)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\frend/.cache\\torch\\hub\\ultralytics_yolov5_master\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot find callable xxxx in hubconf",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m defaultdict\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Charger le modèle YOLOv5 pré-entraîné\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43multralytics/yolov5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mxxxx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Version \"small\" pour rapidité\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fonction de Filtrage et Comptage\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# xxxx : Ajouter dans cette fonction aux tests de catégorie et de volume un test sur la localisation dans l'image avec 4 cadrans (combinant haut, bas, gauche et droit). Ajuster les paramètres en conséquence.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter_objects\u001b[39m(detections, category, size_threshold):\n",
      "File \u001b[1;32mc:\\Users\\frend\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:566\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    563\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(repo_or_dir, force_reload, trust_repo, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    564\u001b[0m                                        verbose\u001b[38;5;241m=\u001b[39mverbose, skip_validation\u001b[38;5;241m=\u001b[39mskip_validation)\n\u001b[1;32m--> 566\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\frend\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:594\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    591\u001b[0m     hubconf_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(hubconf_dir, MODULE_HUBCONF)\n\u001b[0;32m    592\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[1;32m--> 594\u001b[0m     entry \u001b[38;5;241m=\u001b[39m \u001b[43m_load_entry_from_hubconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhub_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    595\u001b[0m     model \u001b[38;5;241m=\u001b[39m entry(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\frend\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\hub.py:348\u001b[0m, in \u001b[0;36m_load_entry_from_hubconf\u001b[1;34m(m, model)\u001b[0m\n\u001b[0;32m    345\u001b[0m func \u001b[38;5;241m=\u001b[39m _load_attr_from_module(m, model)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[1;32m--> 348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot find callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in hubconf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot find callable xxxx in hubconf"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Charger le modèle YOLOv5 pré-entraîné\n",
    "model = torch.hub.load('ultralytics/yolov5', 'xxxx')  # Version \"small\" pour rapidité\n",
    "\n",
    "# Fonction de Filtrage et Comptage\n",
    "# xxxx : Ajouter dans cette fonction aux tests de catégorie et de volume un test sur la localisation dans l'image avec 4 cadrans (combinant haut, bas, gauche et droit). Ajuster les paramètres en conséquence.\n",
    "def filter_objects(detections, category, size_threshold):\n",
    "    \"\"\"\n",
    "    Filtrer les objets en fonction de la catégorie et de la taille minimale.\n",
    "    \n",
    "    :param detections: Résultats de détection (liste de dicts)\n",
    "    :param category: Catégorie à détecter (str)\n",
    "    :param size_threshold: Aire minimale de l'objet (int)\n",
    "    :return: Liste des objets filtrés\n",
    "    \"\"\"\n",
    "    filtered_objects = []\n",
    "    for det in xxxx:\n",
    "        if det['name'] == category:\n",
    "            bbox_width = det['xmax'] - det['xmin']\n",
    "            bbox_height = det['ymax'] - det['xxxx']\n",
    "            area = bbox_width * bbox_height\n",
    "            if area > size_threshold:\n",
    "                filtered_objects.append(det)\n",
    "    return filtered_objects\n",
    "\n",
    "# Implémentation de l'Alarme\n",
    "def trigger_alarm(count, threshold):\n",
    "    \"\"\"\n",
    "    Déclenche une alarme si le seuil est dépassé.\n",
    "    \n",
    "    :param count: Nombre d'objets détectés (int)\n",
    "    :param threshold: Seuil à dépasser (int)\n",
    "    \"\"\"\n",
    "    if count >= threshold:\n",
    "        print(\"!!!!! Alarme !!!!!\")\n",
    "        # On ajoute même un petit bip sonore\n",
    "        duration = 1000  # ms\n",
    "        freq = 440  # Hz\n",
    "        try:\n",
    "            import winsound\n",
    "            winsound.Beep(freq, duration)\n",
    "        except ImportError:\n",
    "            print(\"Bip sonore non disponible sur cette plateforme.\")\n",
    "\n",
    "# Boucle de Détection Temps Réel\n",
    "# Paramètres, à régler\n",
    "CATEGORY = 'pencil'  # Exemple : un crayon\n",
    "SIZE_THRESHOLD = 1000  # Aire minimale\n",
    "THRESHOLD = 5  # Seuil d'alarme\n",
    "\n",
    "# Dictionnaire pour le comptage des objets\n",
    "object_count = defaultdict(int)\n",
    "\n",
    "# Capture vidéo depuis la webcam (à modifier pour lire une vidéo si webcam non disponible)\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convertir l'image pour YOLO\n",
    "    results = model(frame)\n",
    "    detections = results.pandas().xyxy[0].to_dict(orient=\"records\")\n",
    "\n",
    "    # Filtrer les objets\n",
    "    filtered_objects = filter_objects(detections, xxxx, xxxx)\n",
    "\n",
    "    # Mise à jour du comptage\n",
    "    object_count[CATEGORY] = xxxx(filtered_objects)\n",
    "\n",
    "    # Déclencher l'alarme si nécessaire\n",
    "    trigger_alarm(object_count[CATEGORY], THRESHOLD)\n",
    "\n",
    "    # Annoter l'image\n",
    "    for obj in filtered_objects:\n",
    "        cv2.rectangle(frame, (int(obj['xmin']), int(obj['ymin'])), \n",
    "                             (int(obj['xmax']), int(obj['ymax'])), \n",
    "                      (0, 255, 0), 2)\n",
    "        cv2.putText(frame, f\"{obj['name']} {obj['confidence']:.2f}\", \n",
    "                    (int(obj['xmin']), int(obj['ymin']) - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Afficher le flux vidéo\n",
    "    cv2.xxxx('Detection Objets', frame)\n",
    "\n",
    "    # Quitter avec la touche 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    # Dans la boucle principale, ajoutez :\n",
    "\n",
    "\n",
    "# Libérer les ressources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# # Enregistrement\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('test.avi', fourcc, 20.0, (640, 480))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
